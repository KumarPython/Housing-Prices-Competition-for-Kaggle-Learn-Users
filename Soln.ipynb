{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntest=pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the Data-type of each column\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the Percent of null values in each columns\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()/train.shape[0] * 100\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()/test.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the columns in each dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping some useless column. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_train=train['Id']\nID_test=test['Id']\ntest=test.drop(columns=['Id'], axis=1)\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Numerical & Categorical Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_train=[col for col in train.columns if train[col].dtype=='object']\nnum_train=[col for col in train.columns if train[col].dtype!='object']\n# cat_train\nnum_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_test=[col for col in test.columns if test[col].dtype=='object']\nnum_test=[col for col in test.columns if test[col].dtype!='object']\n# cat_test\nnum_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the following Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n### * Continuous Features\n### * Discreet Features\n### *  Year Features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_train =[col for col in num_train if train[col].nunique()>25]\ndis_train =[col for col in num_train if train[col].nunique()<25]\nyea_train =[col for col in train.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n\n# con_train\n# dis_train\n# yea_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_test =[col for col in num_test if test[col].nunique()>25]\ndis_test =[col for col in num_test if test[col].nunique()<25]\nyea_test =[col for col in test.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n# con_test\n# dis_test\nyea_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputing the missing values\n### Missing values are one of the most common problems you can encounter when you try to prepareyour data for machine learning. The reason for the missing values might be human errors,interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nnsi = SimpleImputer(strategy='mean')  # For Numerical Features, will replace MISSING NUMERIC values with MEAN\ncsi = SimpleImputer(strategy='most_frequent')  # For Categorical Features, will replace MISSING CATEGORICAL values with MOST FREQUENT value\n\ntrain[cat_train] = csi.fit_transform(train[cat_train])\ntrain[con_train] = nsi.fit_transform(train[con_train])\ntrain[dis_train] = nsi.fit_transform(train[dis_train])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[cat_test] = csi.fit_transform(test[cat_test])\ntest[con_test] = nsi.fit_transform(test[con_test])\ntest[dis_test] = nsi.fit_transform(test[dis_test])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Log Transform on Continuous Data only"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[con_train]=np.log(train[con_train])\n# test[con_test]= np.log(test[con_test])\ntrain.head()\n# test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming Dates\n### If you transform the date column into the extracted columns, the information of them become disclosed and machine learning algorithms can easily understand them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\ntrain[yea_train]=date.today().year - train[yea_train]\ntest[yea_test]=date.today().year - test[yea_test]\ntrain.head()\n# test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing the Discrete Values.\n### Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\ntrain[dis_train]= ss.fit_transform(train[dis_train])\ntest[dis_test]= ss.fit_transform(test[dis_test])\ntrain.head()\n# test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Categorical Data using Get_Dummies()\n### Machine learning models require all input and output variables to be numeric.This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= pd.get_dummies(train, columns=cat_train, drop_first= True)\ntest1= pd.get_dummies(test, columns=cat_test, drop_first= True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concatenating the Original Dataset & the One after creating Dummies(get_dummies()\n### Get_Dummies() method creates a new DF containing JUST the dummies, MOST People get wrong here)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping the columns already concatenated after Get_Dummies()\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train2.drop(cat_train,axis=1)\ntest=test2.drop(cat_test,axis=1)\ntrain.head()\n# test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.dropna(axis=0,how='any') # I have taken all the necessary features thus dropping null values of unnecessary features\ntest=test.dropna(axis=0,how='any') \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting X & y\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['SalePrice'].iloc[:,0]\n\nX=train.drop(['Id','SalePrice'],axis=1)\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Doing the Train_Test_Split\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\ny_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using GBoost to fit the Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nreg=GradientBoostingRegressor()\nreg.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using the Trained Model to Predict\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict= reg.predict(X_test)\n# predict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scoring the Trained Model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(predict, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some ways you may show Like by\n### Kaggle - Follow me on Kaggle\n### Twitter - https://twitter.com/KumarPython\n### LinkedIn - https://www.linkedin.com/in/kumarpython/\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}